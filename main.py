"""
main.py

ì—„ê²©í•œ AI-Rule ê²€ì¦ ì‹œìŠ¤í…œ - í”„ë¡œì íŠ¸ ì „ì²´ ë³‘ë ¬ ì²˜ë¦¬
"""

import json
import argparse
import subprocess
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Optional

from config.settings import *
from verifiers import StrictVerifier


def find_swift_files(project_path: Path) -> List[Path]:
    """
    í”„ë¡œì íŠ¸ì—ì„œ ëª¨ë“  Swift íŒŒì¼ ì°¾ê¸°
    
    Args:
        project_path: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
    
    Returns:
        Swift íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    return sorted(project_path.rglob("*.swift"))


def extract_ast(swift_file: Path, analyzer_path: Path) -> Optional[Dict]:
    """
    SwiftASTAnalyzerë¡œ AST ì¶”ì¶œ
    
    Args:
        swift_file: Swift íŒŒì¼ ê²½ë¡œ
        analyzer_path: SwiftASTAnalyzer ì‹¤í–‰ íŒŒì¼ ê²½ë¡œ
    
    Returns:
        AST ë°ì´í„° ë˜ëŠ” None
    """
    try:
        result = subprocess.run(
            [str(analyzer_path), str(swift_file)],
            capture_output=True,
            text=True,
            timeout=30,
            encoding='utf-8'
        )
        
        if result.returncode != 0:
            return None
        
        output = result.stdout.strip()
        
        # JSON íŒŒì‹±
        start_idx = output.find('{')
        if start_idx == -1:
            return None
        
        json_str = output[start_idx:]
        ast_data = json.loads(json_str)
        
        # SwiftASTAnalyzer ì¶œë ¥ êµ¬ì¡° ì²˜ë¦¬
        if isinstance(ast_data, dict) and "decisions" in ast_data:
            # decisionsë¥¼ symbolsë¡œ ë³€í™˜
            decisions = ast_data["decisions"]
            symbols = []
            
            for category in ["classes", "structs", "enums", "protocols",
                            "methods", "properties", "variables", "enumCases",
                            "initializers", "deinitializers", "subscripts", "extensions"]:
                if category in decisions and isinstance(decisions[category], list):
                    symbols.extend(decisions[category])
            
            return {"symbols": symbols}
        
        return ast_data
        
    except Exception as e:
        print(f"  âš ï¸  AST ì¶”ì¶œ ì‹¤íŒ¨: {swift_file.name} - {e}")
        return None


def load_llm_identifiers(identifiers_json_path: Path) -> Dict[str, List[str]]:
    """
    LLM ì˜ˆì¸¡ ì‹ë³„ì ë¡œë“œ (íŒŒì¼ë³„)
    
    Args:
        identifiers_json_path: ì‹ë³„ì JSON íŒŒì¼ ê²½ë¡œ
    
    Returns:
        {íŒŒì¼ëª…: [ì‹ë³„ì...]} ë”•ì…”ë„ˆë¦¬
    """
    with open(identifiers_json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
        
        # ë‹¨ì¼ ë¦¬ìŠ¤íŠ¸ í˜•ì‹
        if "identifiers" in data and isinstance(data["identifiers"], list):
            return {"all": data["identifiers"]}
        
        # íŒŒì¼ë³„ í˜•ì‹
        # {"file1.swift": ["id1", "id2"], "file2.swift": [...]}
        return data


def process_single_file(
    swift_file: Path,
    analyzer_path: Path,
    verifier: StrictVerifier,
    llm_identifiers: List[str],
    file_index: int,
    total_files: int,
    min_confidence: float
) -> Dict:
    """
    ë‹¨ì¼ Swift íŒŒì¼ ì²˜ë¦¬
    
    Args:
        swift_file: Swift íŒŒì¼ ê²½ë¡œ
        analyzer_path: SwiftASTAnalyzer ê²½ë¡œ
        verifier: StrictVerifier ì¸ìŠ¤í„´ìŠ¤
        llm_identifiers: LLM ì˜ˆì¸¡ ì‹ë³„ì
        file_index: íŒŒì¼ ì¸ë±ìŠ¤
        total_files: ì „ì²´ íŒŒì¼ ìˆ˜
        min_confidence: ìµœì†Œ ì‹ ë¢°ë„
    
    Returns:
        ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print(f"[{file_index}/{total_files}] ì²˜ë¦¬ ì¤‘: {swift_file.name}")
    
    # AST ì¶”ì¶œ
    ast_data = extract_ast(swift_file, analyzer_path)
    
    if not ast_data:
        print(f"  âš ï¸  AST ì¶”ì¶œ ì‹¤íŒ¨")
        return {
            "file": str(swift_file),
            "success": False,
            "error": "AST extraction failed"
        }
    
    # ê²€ì¦
    results = verifier.verify(
        ast_data=ast_data,
        llm_identifiers=llm_identifiers,
        min_confidence=min_confidence
    )
    
    # ìµœì¢… ì œì™¸ ì‹ë³„ì
    exclusions = verifier.get_final_exclusions(results, min_confidence)
    
    print(f"  âœ“ ì™„ë£Œ: {len(exclusions)}ê°œ ì œì™¸")
    
    return {
        "file": str(swift_file),
        "success": True,
        "exclusions": exclusions,
        "total_predictions": len(results),
        "found_in_ast": sum(1 for r in results if r.found_in_ast),
        "rule_matched": sum(1 for r in results if r.rule_matches),
        "details": [
            {
                "identifier": r.identifier,
                "found_in_ast": r.found_in_ast,
                "rule_matched": len(r.rule_matches) > 0,
                "matched_rules": [m.rule_id for m in r.rule_matches],
                "final_decision": r.final_decision,
                "confidence": r.confidence,
                "reasoning": r.reasoning
            }
            for r in results
        ]
    }


def print_summary(all_results: List[Dict]):
    """ì „ì²´ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    print("\n" + "=" * 70)
    print("ğŸ“Š ì „ì²´ ê²€ì¦ ê²°ê³¼")
    print("=" * 70)
    
    total_files = len(all_results)
    success_files = sum(1 for r in all_results if r.get("success", False))
    failed_files = total_files - success_files
    
    print(f"\nâœ“ ì´ íŒŒì¼: {total_files}ê°œ")
    print(f"âœ“ ì„±ê³µ: {success_files}ê°œ")
    print(f"âœ“ ì‹¤íŒ¨: {failed_files}ê°œ")
    
    if success_files > 0:
        # í†µê³„
        total_predictions = sum(r.get("total_predictions", 0) for r in all_results if r.get("success"))
        total_found = sum(r.get("found_in_ast", 0) for r in all_results if r.get("success"))
        total_rule_matched = sum(r.get("rule_matched", 0) for r in all_results if r.get("success"))
        total_exclusions = sum(len(r.get("exclusions", [])) for r in all_results if r.get("success"))
        
        print(f"\nğŸ“ˆ ì „ì²´ í†µê³„:")
        print(f"  â€¢ ì´ LLM ì˜ˆì¸¡: {total_predictions}ê°œ")
        print(f"  â€¢ AST ë§¤ì¹­: {total_found}/{total_predictions}ê°œ ({total_found/total_predictions*100:.1f}%)")
        print(f"  â€¢ Rule ë§¤ì¹­: {total_rule_matched}/{total_found}ê°œ ({total_rule_matched/total_found*100:.1f}% if total_found > 0 else 0%)")
        print(f"  â€¢ ìµœì¢… ì œì™¸: {total_exclusions}ê°œ")
        
        # í™˜ê°ë¥ 
        hallucination = total_predictions - total_found
        print(f"\nâš ï¸  í™˜ê°ë¥ : {hallucination}/{total_predictions}ê°œ ({hallucination/total_predictions*100:.1f}%)")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    parser = argparse.ArgumentParser(
        description="ì—„ê²©í•œ AI-Rule ê²€ì¦ ì‹œìŠ¤í…œ - í”„ë¡œì íŠ¸ ì „ì²´ ë³‘ë ¬ ì²˜ë¦¬"
    )
    parser.add_argument(
        '--project',
        type=str,
        required=True,
        help='Swift í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ'
    )
    parser.add_argument(
        '--identifiers',
        type=str,
        required=True,
        help='LLM ì˜ˆì¸¡ ì‹ë³„ì JSON íŒŒì¼ ê²½ë¡œ'
    )
    parser.add_argument(
        '--analyzer',
        type=str,
        default='SwiftASTAnalyzer/.build/release/SwiftASTAnalyzer',
        help='SwiftASTAnalyzer ì‹¤í–‰ íŒŒì¼ ê²½ë¡œ'
    )
    parser.add_argument(
        '--rules',
        type=str,
        default=str(RULES_YAML),
        help='Rule YAML íŒŒì¼ ê²½ë¡œ'
    )
    parser.add_argument(
        '--output',
        type=str,
        default=None,
        help='ê²°ê³¼ ì €ì¥ ê²½ë¡œ'
    )
    parser.add_argument(
        '--workers',
        type=int,
        default=5,
        help='ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸: 5)'
    )
    parser.add_argument(
        '--min-confidence',
        type=float,
        default=MIN_CONFIDENCE,
        help='ìµœì†Œ ì‹ ë¢°ë„ (ê¸°ë³¸: 1.0)'
    )
    
    args = parser.parse_args()
    
    # ê²½ë¡œ í™•ì¸
    project_path = Path(args.project)
    identifiers_path = Path(args.identifiers)
    analyzer_path = Path(args.analyzer)
    rules_path = Path(args.rules)
    
    if not project_path.exists():
        print(f"âŒ í”„ë¡œì íŠ¸ ê²½ë¡œ ì—†ìŒ: {project_path}")
        return 1
    
    if not identifiers_path.exists():
        print(f"âŒ ì‹ë³„ì íŒŒì¼ ì—†ìŒ: {identifiers_path}")
        return 1
    
    if not analyzer_path.exists():
        print(f"âŒ SwiftASTAnalyzer ì—†ìŒ: {analyzer_path}")
        print(f"   ë¹Œë“œ í•„ìš”: cd SwiftASTAnalyzer && swift build -c release")
        return 1
    
    if not rules_path.exists():
        print(f"âŒ Rule íŒŒì¼ ì—†ìŒ: {rules_path}")
        return 1
    
    # Swift íŒŒì¼ ì°¾ê¸°
    print("\nğŸ” Swift íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
    swift_files = find_swift_files(project_path)
    
    if not swift_files:
        print(f"âŒ Swift íŒŒì¼ ì—†ìŒ: {project_path}")
        return 1
    
    print(f"âœ“ {len(swift_files)}ê°œ Swift íŒŒì¼ ë°œê²¬")
    
    # LLM ì‹ë³„ì ë¡œë“œ
    print("\nğŸ“¥ LLM ì˜ˆì¸¡ ë¡œë“œ ì¤‘...")
    identifiers_data = load_llm_identifiers(identifiers_path)
    
    # ë‹¨ì¼ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” íŒŒì¼ë³„ ì²˜ë¦¬
    if "all" in identifiers_data:
        llm_identifiers = identifiers_data["all"]
        print(f"âœ“ {len(llm_identifiers)}ê°œ ì‹ë³„ì ë¡œë“œ (ì „ì²´ ê³µí†µ)")
    else:
        # íŒŒì¼ë³„ ì‹ë³„ì (ì¶”í›„ êµ¬í˜„)
        llm_identifiers = []
        for ids in identifiers_data.values():
            llm_identifiers.extend(ids)
        llm_identifiers = list(set(llm_identifiers))
        print(f"âœ“ {len(llm_identifiers)}ê°œ ì‹ë³„ì ë¡œë“œ (íŒŒì¼ë³„ ë³‘í•©)")
    
    # Verifier ì´ˆê¸°í™”
    print("\nâš™ï¸  Verifier ì´ˆê¸°í™” ì¤‘...")
    verifier = StrictVerifier(rules_path)
    print(f"âœ“ {len(verifier.rule_engine.get_all_rules())}ê°œ Rule ë¡œë“œ")
    
    # ë³‘ë ¬ ì²˜ë¦¬
    print("\n" + "=" * 70)
    print(f"ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (ì›Œì»¤: {args.workers}ê°œ)")
    print("=" * 70 + "\n")
    
    start_time = time.time()
    all_results = []
    
    with ThreadPoolExecutor(max_workers=args.workers) as executor:
        futures = {}
        
        for i, swift_file in enumerate(swift_files, 1):
            future = executor.submit(
                process_single_file,
                swift_file,
                analyzer_path,
                verifier,
                llm_identifiers,
                i,
                len(swift_files),
                args.min_confidence
            )
            futures[future] = swift_file
        
        # ê²°ê³¼ ìˆ˜ì§‘
        for future in as_completed(futures):
            result = future.result()
            all_results.append(result)
    
    elapsed_time = time.time() - start_time
    
    # ê²°ê³¼ ìš”ì•½
    print_summary(all_results)
    
    print(f"\nâ±ï¸  ì²˜ë¦¬ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    print(f"âš¡ í‰ê·  ì†ë„: {len(swift_files)/elapsed_time:.2f} files/sec")
    
    # ê²°ê³¼ ì €ì¥
    if args.output:
        output_path = Path(args.output)
    else:
        output_path = RESULTS_DIR / f"verification_{int(time.time())}.json"
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    summary = {
        "project": str(project_path),
        "total_files": len(swift_files),
        "success_files": sum(1 for r in all_results if r.get("success")),
        "failed_files": sum(1 for r in all_results if not r.get("success")),
        "processing_time_seconds": elapsed_time,
        "files_per_second": len(swift_files) / elapsed_time,
        "workers": args.workers,
        "results": all_results
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥: {output_path}")
    
    return 0


if __name__ == "__main__":
    exit(main())
